%
% eigenwerte.tex
%
% Begriff des Eigenwertes und Eigenvektors
%
\section{Eigenwerte und Eigenvektoren
\label{buch:section:eigenwerte-und-eigenvektoren}}
In diesem Abschnitt betrachten wir Vektorräume $V=\Bbbk^n$ über einem
beliebigen Körper $\Bbbk$ und quadratische Matrizen
$A\in M_n(\Bbbk)$.
In den meisten Anwendungen wird $\Bbbk=\mathbb{R}$ sein.
Da aber in $\mathbb{R}$ nicht alle algebraischen Gleichungen lösbar sind,
ist es manchmal notwendig, den Vektorraum zu erweitern um zum Beispiel
auf dem Umweg über komplexe Zahlen
Eigenschaften der Matrix $A$ abzuleiten.

\begin{definition}
\label{buch:eigenwerte:def:evew}
\label{buch:eigenwerte:def:spektrum}
Ein Vektor $v\in V$ heisst {\em Eigenvektor} von $A$ zum {\em Eigenwert}
\index{Eigenwert}%
\index{Eigenvektor}%
$\lambda\in\Bbbk$, wenn $v\ne 0$ und $Av=\lambda v$ gilt.
Die Menge
\[
\operatorname{Sp}(A)
=
\{\lambda\in\mathbb{C}\,|\, \text{$\lambda$ ist Eigenwert von $A$}\}
\]
heisst das {\em Spektrum} von $A$.
\index{Spektrum}%
\end{definition}

Die Bedingung $v\ne 0$ dient dazu, pathologische Situationen auszuschliessen.
Für den Nullvektor gilt $A0=\lambda 0$ für jeden beliebigen Wert von
$\lambda\in\Bbbk$.
Würde man $v=0$ zulassen, wäre jede Zahl in $\Bbbk$ ein Eigenwert,
ein Eigenwert von $A$ wäre nichts besonderes.
Ausserdem wäre $0$ ein Eigenvektor zu jedem beliebigen Eigenwert.

Eigenvektoren sind nicht eindeutig bestimmt, jedes von $0$ verschiedene
Vielfache von $v$ ist ebenfalls ein Eigenvektor.
Zu einem Eigenwert kann man also einen Eigenvektor mit 
geeigneten Eigenschaften finden, zum Beispiel kann man für $\Bbbk = \mathbb{R}$
Eigenvektoren auf Länge $1$ normieren.
Im Folgenden werden wir oft die abkürzend linear unabhängige Eigenvektoren
einfach als ``verschiedene'' Eigenvektoren bezeichnen.

Wenn $v$ ein Eigenvektor von $A$ zum Eigenwert $\lambda$ ist, dann kann
man ihn mit zusätzlichen Vektoren $v_2,\dots,v_n$ zu einer Basis
$\mathcal{B}=\{v,v_2,\dots,v_n\}$
von $V$ ergänzen.
Die Vektoren $v_k$ mit $k=2,\dots,n$ werden von $A$ natürlich auch
in den Vektorraum $V$ abgebildet, können also als Linearkombinationen
\[
Av = a_{1k}v + a_{2k}v_2 + a_{3k}v_3 + \dots a_{nk}v_n
\]
dargestellt werden.
In der Basis $\mathcal{B}$ bekommt die Matrix $A$ daher die Form
\[
A'
=
\begin{pmatrix}
\lambda&a_{12}&a_{13}&\dots &a_{1n}\\
    0  &a_{22}&a_{23}&\dots &a_{2n}\\
    0  &a_{32}&a_{33}&\dots &a_{3n}\\
\vdots &\vdots&\vdots&\ddots&\vdots\\
    0  &a_{n2}&a_{n3}&\dots &a_{nn}
\end{pmatrix}.
\]
Bereits ein einzelner Eigenwert und ein zugehöriger Eigenvektor
ermöglichen uns also, die Matrix in eine etwas einfachere Form
zu bringen.

\begin{definition}
Für $\lambda\in\Bbbk$ heisst
\[
E_\lambda
=
\{ v\;|\; Av=\lambda v\}
\]
der {\em Eigenraum} zum Eigenwert $\lambda$.
\index{Eigenraum}%
\end{definition}

Der Eigenraum $E_\lambda$ ist ein Unterraum von $V$, denn wenn
$u,v\in E_\lambda$, dann ist
\[
A(su+tv)
=
sAu+tAv
=
s\lambda u + t\lambda v
=
\lambda(su+tv),
\]
also ist auch $su+tv\in E_\lambda$.
Der Spezialfall $E_\lambda = \{0\}=0$ bedeutet natürlich, dass $\lambda$ gar kein
Eigenwert ist.

\begin{satz}
Wenn $\dim E_\lambda=n$ ist, dann ist $A=\lambda I$.
\end{satz}

\begin{proof}[Beweis]
Da $V$ ein $n$-dimensionaler Vektoraum ist, ist $E_\lambda=V$.
Jeder Vektor $v\in V$ erfüllt also die Bedingung $Av=\lambda v$,
oder $A=\lambda I$.
\end{proof}

Wenn man die Eigenräume von $A$ kennt, dann kann man auch die Eigenräume
von $A+\mu E$ berechnen.
Ein Vektor $v\in E_\lambda$ erfüllt
\[
Av=\lambda v
\qquad\Rightarrow\qquad
(A+\mu)v = \lambda v + \mu v
=
(\lambda+\mu)v,
\]
somit ist $v$ ein Eigenvektor von $A+\mu I$ zum Eigenwert $\lambda+\mu$.
Insbesondere können wir statt die Eigenvektoren von $A$ zum Eigenwert $\lambda$
zu studieren, auch die Eigenvektoren zum Eigenwert $0$ von $A-\lambda I$
untersuchen.

%
% Invariante Räume
%
\subsection{Verallgemeinerte Eigenräume
\label{buch:subsection:verallgemeinerte-eigenraeume}}
Wenn $\lambda$ ein Eigenwert der Matrix $A$ ist, dann ist
ist $A-\lambda I$ injektiv und $\ker(A-\lambda I)\ne 0$.
Man kann daher die invarianten Unterräume $\mathcal{K}(A-\lambda I)$
und $\mathcal{J}(A-\lambda I)$ bilden.

\begin{beispiel}
Wir untersuchen die Matrix
\[
A
=
\begin{pmatrix}
1&1&-1&0\\
0&3&-1&1\\
0&2& 0&1\\
0&0& 0&2
\end{pmatrix}
\]
Man kann zeigen, dass $\lambda=1$ ein Eigenwert ist.
Wir suchen die Zerlegung des Vektorraums $\mathbb{R}^4$ in invariante
Unterräume $\mathcal{K}(A-I)$ und $\mathcal{J}(A-I)$.
Die Matrix $B=A-I$ ist
\[
B
=
\begin{pmatrix}
0&1&-1&0\\
0&2&-1&1\\
0&2&-1&1\\
0&0& 0&2
\end{pmatrix}
\]
und wir berechnen davon die vierte Potenz
\[
D=B^4=(A-E)^4
=
\begin{pmatrix}
0&0& 0&0\\
0&2&-1&4\\
0&2&-1&4\\
0&0& 0&1
\end{pmatrix}.
\]
Daraus kann man ablesen, dass das Bild $\operatorname{im}D$
von $D$ die Basis
\[
b_1
=
\begin{pmatrix}
0\\0\\0\\1
\end{pmatrix}
, \qquad
b_2
=
\begin{pmatrix}
0\\1\\1\\0
\end{pmatrix}
\]
hat.
Für den Kern von $D$ können wir zum Beispiel die Basisvektoren
\[
b_3
=
\begin{pmatrix}
0\\1\\2\\0
\end{pmatrix}
,\qquad
b_4
=
\begin{pmatrix}
1\\0\\0\\0
\end{pmatrix}
\]
verwenden.

Als erstes überprüfen wir, ob diese Basisvektoren tatsächlich invariante
Unterräume sind.
Für $\mathcal{J}(A-I) = \langle b_1,b_2\rangle$
berechnen wir
\begin{align*}
(A-I)b_1
&=
\begin{pmatrix} 0\\4\\4\\1 \end{pmatrix}
=
4b_2+b_1,
\\
(A-I)b_2
&=
\begin{pmatrix} 0\\1\\1\\0 \end{pmatrix}
=
b_2.
\end{align*}
Dies beweist, dass $\mathcal{J}(A-I)$ invariant ist.
In dieser Basis hat die von $A-I$ beschriebene lineare Abbildung
auf $\mathcal{J}(A-I)$ die Matrix
\[
A_{\mathcal{J}(A-I)}
=
\begin{pmatrix}
1&4\\
0&1
\end{pmatrix}.
\]

Für den Kern $\mathcal{K}(A-I)$ findet man analog
\[
\left.
\begin{aligned}
Ab_3
&=
-b_4
\\
Ab_4
&=0
\end{aligned}
\quad\right\}
\qquad\Rightarrow\qquad
A_{\mathcal{K}(A-I)}
=
\begin{pmatrix}
0&-1\\
0& 0
\end{pmatrix}.
\]
In der Basis $\mathcal{B}=\{b_1,b_2,b_3,b_4\}$ hat $A$ die Matrix
in Blockform
\[
A'
=
\left(
\begin{array}{cc|cr}
2&4& & \\
0&2& & \\
\hline
 & &1&-1\\
 & &0& 1
\end{array}\right),
\]
die Blöcke gehören zu den invarianten Unterräumen $\mathcal{K}(A-I)$
und $\mathcal{K}(A-I)$.
Die aus $A-E$ gewonnen invarianten Unterräume sind offenbar auch invariante
Unterräume für $A$.
\end{beispiel}

\begin{definition}
Ist $A$ eine Matrix mit Eigenwert $\lambda$, dann heisst der invariante
Unterraum
\[
\mathcal{E}_{\lambda}(A)
=
\mathcal{K}(A-\lambda I)
\]
der {\em verallgemeinerte Eigenraum} von $A$.
\index{verallgemeinerter Eigenraum}%
\index{Eigenraum, verallgemeinerter}%
\end{definition}

Es ist klar, dass
$E_\lambda(A)=\ker (A-\lambda I)\subset\mathcal{E}_{\lambda}(A)$.

\subsection{Zerlegung in invariante Unterräume
\label{buch:subsection:zerlegung-in-invariante-unterraeume}}
Wenn $\lambda$ kein Eigenwert von $A$ ist, dann ist $A-\lambda I$
injektiv und damit $\ker(A-\lambda I)=0$.
Es folgt, dass $\mathcal{K}^i(A-\lambda I)=0$ und daher auch
$\mathcal{J}^i(A-\lambda I)=V$.
Die Zerlegung in invariante Unterräume $\mathcal{J}(A-\lambda I)$ und
$\mathcal{E}_\lambda(A)=\mathcal{K}(A-\lambda I)$ liefert in diesem Falle also nichts Neues.

Für einen Eigenwert $\lambda_1$ von $A$ dagegen erhalten wir die Zerlegung
\[
V
=
\mathcal{E}_{\lambda_1}(A)
\oplus
\underbrace{\mathcal{J}(A-\lambda_1 I)}_{\displaystyle =V_2},
\]
wobei $\mathcal{E}_{\lambda_1}(A)\ne 0$ ist.
Die Matrix $A-\lambda_1 I$ eingeschränkt auf $\mathcal{E}_{\lambda_1}(A)$ ist
nilpotent.
Man kann sagen, auf dem Unterraum $\mathcal{E}_{\lambda_i}(A)$ hat 
$A$ die Form $\lambda_1 I + N$, wobei $N$ nilpotent ist.

Die Zerlegung in invariante Unterräume ist zwar mit Hilfe von $A-\lambda_1I$
gewonnen worden, ist aber natürlich auch eine Zerlegung in invariante 
Unterräume für $A$.
Wir können daher das Problem auf $V_2$ einschränken und nach einem weiteren
Eigenwert $\lambda_2$ von $A$ in $V_2$ suchen.
Dieser neue Eigenwert liefert eine Zerlegung von $V_2$
in invariante Unterräume.
Indem wir so weiterarbeiten, bis wir den ganzen Raum ausgeschöpft haben,
können wir eine Zerlegung des ganzen Raumes $V$ finden, so dass $A$ auf
jedem einzelnen Summanden die sehr einfach Form
``$\lambda I + \text{nilpotent}$'' hat:

\begin{satz}
\label{buch:eigenwerte:satz:zerlegung-in-eigenraeume}
Sei $V$ ein $\Bbbk$-Vektorraum und $f$ eine lineare Abbildung mit Matrix
$A$ derart, dass alle Eigenwerte $\lambda_1,\dots,\lambda_l$ von $A$
in $\Bbbk$ sind.
Dann gibt es eine Zerlegung von $V$ in verallgemeinerte Eigenräume
\[
V
=
\mathcal{E}_{\lambda_1}(A)
\oplus
\mathcal{E}_{\lambda_2}(A)
\oplus
\dots
\oplus
\mathcal{E}_{\lambda_l}(A).
\]
Die Einschränkung von $A-\lambda_{i}I$ auf den Eigenraum 
$\mathcal{E}_{\lambda_i}(A)$ ist nilpotent.
\end{satz}

\subsection{Das charakteristische Polynom
\label{buch:subsection:das-charakteristische-polynom}}
Ein Eigenvektor von $A$ erfüllt $Av=\lambda v$ oder gleichbedeutend
$(A-\lambda I)v=0$, er ist also eine nichttriviale Lösung des homogenen
Gleichungssystems mit Koeffizientenmatrix $A-\lambda I$. 
Ein Eigenwert ist also ein Skalar derart, dass $A-\lambda I$
singulär ist.
Ob eine Matrix singulär ist, kann mit der Determinante festgestellt
werden.
Die Eigenwerte einer Matrix $A$ sind daher die Nullstellen
von $\det(A-\lambda I)$.

\begin{definition}
Das {\em charakteristische Polynom}
\[
\chi_A(x)
=
\det (A-x I)
=
\left|
\begin{matrix}
a_{11}-x & a_{12}   & \dots  & a_{1n} \\
a_{21}   & a_{22}-x & \dots  & a_{2n} \\
\vdots   &\vdots    &\ddots  & \vdots \\
a_{n1}   & a_{n2}   &\dots   & a_{nn}-x
\end{matrix}
\right|.
\]
der Matrix $A$ ist ein Polynom vom Grad $n$ mit Koeffizienten in $\Bbbk$.
\index{charakteristisches Polynom}%
\index{Polynome, charakteristisches}%
\end{definition}

Findet man eine Nullstelle $\lambda\in\Bbbk$ von $\chi_A(x)$,
dann ist die Matrix $A-\lambda I\in M_n(\Bbbk)$ und mit dem Gauss-Algorithmus
kann man auch mindestens einen Vektor $v\in \Bbbk^n$ finden,
der $Av=\lambda v$ erfüllt.
Eine Dreiecksmatrix der Form 
\[
A=\begin{pmatrix}
\lambda&   *   &   *   &   *   &\dots &*\\
   0   &\lambda&   *   &   *   &\dots &*\\
   0   &   0   &\lambda&   *   &\dots &*\\
   0   &   0   &   0   &\lambda&\dots &*\\
\vdots &\vdots &\vdots &       &\ddots&\vdots\\
   0   &   0   &   0   &   0   &\dots &\lambda
\end{pmatrix}
\]
hat
\[
\chi_A(x)
=
\left|
\begin{matrix}
\lambda-x &     *     &     *     &      &    *    &     *   \\
          & \lambda-x &     *     &      &    *    &     *   \\
          &           & \lambda-x &      &    *    &     *   \\
          &           &           &\ddots&    *    &     *   \\
          &           &           &      &\lambda-x&     *   \\
          &           &           &      &         &\lambda-x
\end{matrix}
\right|
=
(\lambda-x)^n
=
(-1)^n (x-\lambda)^n
\]
als charakteristisches Polynom, welches $\lambda$ als einzige
Nullstelle hat.
Wenn die Einträge oberhalb der Diagonalen nicht alle 0 sind,
dann hat der Eigenraum der Matrix Dimension, die keiner ist als
$n$.
Man kann also im Allgemeinen für jede Nullstelle des charakteristischen
Polynoms nicht mehr als einen Eigenvektor (d.~h.~einen eindimensionalen
Eigenraum) erwarten.

Wenn das charakteristische Polynom von $A$ keine Nullstellen in $\Bbbk$ hat,
dann kann es auch keine Eigenvektoren in $\Bbbk^n$ geben.
Gäbe es nämlich einen solchen Vektor, dann müsste eine der Komponenten
des Vektors von $0$ verschieden sein.
Wir nehmen an, dass es die Komponente in Zeile $k$ ist.
Die Komponente $v_k$ kann man auf zwei Arten berechnen, einmal als
die $k$-Komponenten von $Av$ und einmal als $k$-Komponente von $\lambda v$:
\[
a_{k1}v_1+\dots+a_{kn}v_n = \lambda v_k.
\]
Da $v_k\ne 0$ kann man nach $\lambda$ auflösen und erhält
\[
\lambda = \frac{a_{k1}v_1+\dots + a_{kn}v_n}{v_k}.
\]
Alle Terme auf der rechten Seite sind in $\Bbbk$ und werden nur mit
Körperoperationen in $\Bbbk$ verknüpft, also muss auch $\lambda\in\Bbbk$
sein, im Widerspruch zur Annahme.

Durch Hinzufügen von geeigneten Elementen können wir immer zu einem 
Körper $\Bbbk'$ übergehen, in dem das charakteristische Polynom
in Linearfaktoren zerfällt.
\index{Linearfaktor}%
Für reelle Matrizen kann man zum Beispiel zu $\mathbb{C}$ übergehen,
da ein reelles Polynom alle Nullstellen in $\mathbb{C}$ hat.
In diesem Körper $\Bbbk'$ kann man jetzt das homogene lineare Gleichungssystem
mit Koeffizientenmatrix $A-\lambda I$ lösen und damit mindestens 
einen Eigenvektor $v$ für jeden Eigenwert finden.
Die Komponenten von $v$ liegen in $\Bbbk'$, und mindestens eine davon kann
nicht in $\Bbbk$ liegen.
Das bedeutet aber nicht, dass man diese Vektoren nicht für theoretische
Überlegungen über von $\Bbbk'$ unabhängige Eigenschaften der Matrix $A$ machen.
Das folgende Beispiel soll diese Idee illustrieren.

\begin{beispiel}
Wir arbeiten in diesem Beispiel über dem Körper $\Bbbk=\mathbb{Q}$.
Die Matrix
\[
A=\begin{pmatrix}
-4&7\\
-2&4
\end{pmatrix}
\in
M_2(\mathbb{Q})
\]
hat das charakteristische Polynom
\[
\chi_A(x)
=
\left|
\begin{matrix}
-4-x&7\\-2&4-x
\end{matrix}
\right|
=
(-4-x)(4-x)-7\cdot(-2)
=
-16+x^2+14
=
x^2-2.
\]
Die Nullstellen sind $\pm\sqrt{2}$ und damit nicht in $\mathbb{Q}$.
Wir gehen daher über zum Körper $\mathbb{Q}(\!\sqrt{2})$, in dem
sich zwei Nullstellen $\lambda=\pm\sqrt{2}$ finden lassen.
Zu jedem Eigenwert lässt sich auch ein Eigenvektor
$v_{\pm\sqrt{2}}\in \mathbb{Q}(\!\sqrt{2})^2$, und unter Verwendung dieser
Basis bekommt die Matrix $A'=TAT^{-1}$ Diagonalform.
Die Transformationsmatrix $T$ enthält Matrixelemente aus
$\mathbb{Q}(\!\sqrt{2})$, die nicht in $\mathbb{Q}$ liegen.
Die Matrix $A$ lässt sich also über dem Körper $\mathbb{Q}(\!\sqrt{2})$
diagonalisieren, nicht aber über dem Körper $\mathbb{Q}$.

Da $A'$ Diagonalform hat mit $\pm\sqrt{2}$ auf der Diagonalen, folgt
$A^{\prime 2} = 2I$, die Matrix $A'$ erfüllt also die Gleichung
\begin{equation}
A^{\prime 2}-I= \chi_{A}(A) = 0.
\label{buch:grundlagen:eqn:cayley-hamilton-beispiel}
\end{equation}
Die Gleichung~\ref{buch:grundlagen:eqn:cayley-hamilton-beispiel}
wurde zwar in $\mathbb{Q}(\!\sqrt{2})$ hergeleitet, aber in ihr kommen
keine Koeffizienten aus $\mathbb{Q}(\!\sqrt{2})$ vor, die man nicht auch
in $\mathbb{Q}$ berechnen könnte.
Sie gilt daher ganz allgemein, also $A^2-I=0$.
Dies is ein Spezialfall des Satzes von Cayley-Hamilton
\index{Cayley-Hamilton, Satz von}%
\index{Satz von Cayley-Hamilton}%
(Satz~\ref{buch:normalformen:satz:cayley-hamilton})
welcher besagt, dass jede Matrix $A$ eine Nullstelle ihres 
charakteristischen Polynoms ist: $\chi_A(A)=0$.
\end{beispiel}

\begin{beispiel}
Die Matrix
\[
A=\begin{pmatrix}
32&-41\\
24&-32
\end{pmatrix}
\in
M_2(\mathbb{R})
\]
über dem Körper $\Bbbk = \mathbb{R}$
hat das charakteristische Polynom
\[
\det(A-xI)
=
\left|
\begin{matrix}
32-x&-41  \\
25  &-32-x
\end{matrix}
\right|
=
(32-x)(-32-x)-25\cdot(-41)
=
x^2-32^2 + 1025
=
x^2+1.
\]
Die charakteristische Gleichung $\chi_A(x)=0$ hat in $\mathbb{R}$
keine Lösungen, daher gehen wir zum Körper $\Bbbk'=\mathbb{C}$ über,
in dem dank dem Fundamentalsatz \ref{buch:zahlen:satz:fundamentalsatz}
der Algebra alle Nullstellen zu finden sind, sie sind $\pm i$.
In $\mathbb C$ lassen sich dann auch Eigenvektoren finden, man muss dazu die
folgenden homogenen linearen Gleichungssyteme in Tableauform lösen:
\begin{align*}
\begin{tabular}{|>{$}c<{$}>{$}c<{$}|}
\hline
32-i&-41\\
25  &-32-i\\
\hline
\end{tabular}
&
\rightarrow
\begin{tabular}{|>{$}c<{$}>{$}c<{$}|}
\hline
1 & t\\
0 &  0 \\
\hline
\end{tabular}
&
\begin{tabular}{|>{$}c<{$}>{$}c<{$}|}
\hline
32+i&-41\\
25  &-32+i\\
\hline
\end{tabular}
&
\rightarrow
\begin{tabular}{|>{$}c<{$}>{$}c<{$}|}
\hline
1 & \overline{t}\\
0 &  0 \\
\hline
\end{tabular},
\intertext{wobei wir $t=-41/(32-i) =-41(32+i)/1025= -1.28 -0.04i = (64-1)/50$
abgekürzt haben.
Die zugehörigen Eigenvektoren sind}
v_i&=\begin{pmatrix}t\\-1\end{pmatrix}
&
v_{-i}&=\begin{pmatrix}\overline{t}\\-1\end{pmatrix}.
\end{align*}
Mit den Vektoren $v_i$ und $v_{-i}$ als Basis kann die Matrix $A$ als
komplexe Matrix, also mit komplexem $T$ in die komplexe Diagonalmatrix 
$A'=\operatorname{diag}(i,-i)$ transformiert werden.
Wieder kann man sofort ablesen, dass $A^{\prime2}+I=0$, und wieder kann
man schliessen, dass für die relle Matrix $A$ ebenfalls $\chi_A(A)=0$
gelten muss.
\end{beispiel}




